# 吴侃真的工作记录

## 2020/7/6 - 2020/7/9

完成了前端UI的设计、布局、样式、组件等工作

## 2020/7/10

增加了页面跳转逻辑和与后端的交互

## 2020/7/13

修复了描述的文本溢出问题，增加了Skeleton设计，增加了日间夜间主题切换功能，将前端部署至服务器上

## 2020/7/14

对前端部分函数进行了测试，探究关键词算法

## 2020/7/15

搭建 jest & enzyme 测试环境，编写了部分组件的测试用例。但覆盖率测试与enzyme的渲染函数有bug，并初步探究了原因

## 2020/7/16

jest能过而jest --coverage不行的原因是react脚手架的版本过高，需要回退版本；

enzyme渲染出的react组件在setState后无法正确更新，可以通过传入Promise来控制；但许多enzyme文档中有的方法还是无法调用，初步估计是react版本与enzyme-react-adapter的版本不兼容的问题

重新讨论并规划了新的项目目标，我的工作转为写中文维基百科的爬虫

## 2020/7/17 - 2020/7/19

使用 Scrapy 搭建爬虫框架

学习 MediaPedia 的 API

hack了Wikipedia-API库的源码，利用了其解析wikicode的功能

完成了词条的可持续化爬取，并能正确区分词条的类型，能通过重定向获取词条的真实名字，能将文本根据目录结构保存为json

初步探究了Scrapy-Redis搭建分布式爬虫的功能

## 2020/7/20

实现了词条目录中文本超链接，并能正确重定向（但为了减少http请求降低了一部分精确性）

在服务器上部署了 MongoDB 与 Neo4j

爬虫能够正确地将数据格式化存储在数据库中

## 2020/7/21

优化了 Neo4j 的存储速度

搭建了 Redis 用来进行分布式调度与断点续传

尝试优化了代理的配置，虽然效果不好

## 2020/7/22

进行了一系列实验后发现了重载 Scrapy 的 DUPEFILTER_CLASS 后会出现速度问题

在 scrapy-redis, scrapy 的 GitHub 主页上和 StackOverflow 上提出了问题

最终探索出了一个使用 Redis 来进行指纹去重且支持断点续传的方案（在DownloaderMiddleware中进行处理）

[Scarpy-redis slows down item pipelines](https://stackoverflow.com/questions/63026873/scarpy-redis-slows-down-item-pipelines)

[Item pipelines are slowed down](https://github.com/rmax/scrapy-redis/issues/174)

[Customized dupefilter slows down item pipeline's speed](https://github.com/scrapy/scrapy/issues/4689)

## 2020/7/23

编写了具有断点续爬，掉线重试并记录功能的爬虫

对爬虫进行了部署与监控

## 2020/7/24

测试不同设置下爬虫速度，最终选择最优配置

搭建了 jest & enzyme 的前端测试环境

## 2020/7/27

原先梯子挂了，重新用搬瓦工VPS搭建了 V2Ray + WebSocket + TLS + Nginx 代理

测试了 jest & enzyme / React Test Utilities / React Test Library 的可用性，最终选择后者

## 2020/7/28

编写新爬虫爬取了中文维基百科的所有标题页的简介，用来防止原爬虫的数据丢失情况

## 2020/7/29

优化梯子的并发性能

优化爬虫的设置，提高爬取速度

对 MediaWiki API 获取所有页面的问题进行了探究

[How to Get the Pageids and Titles of All Wikipedia's Content Pages Through MediaWiki API?](https://stackoverflow.com/questions/63149327/how-to-get-the-pageids-and-titles-of-all-wikipedias-content-pages-through-media)

## 2020/7/30

增加了 pymongo 遍历功能

增加了数据库迁移与导出功能
